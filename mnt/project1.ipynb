{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jupyter Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Ingestion and Combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"input/sample_nyc_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"NYC Taxi Analysis\").getOrCreate()\n",
    "\n",
    "df_spark = spark.read.csv(\"input/sample_nyc_data.csv\", header=True, inferSchema=True).cache()\n",
    "\n",
    "df_selected = df_spark.select(\"medallion\", \n",
    "                        \"pickup_datetime\", \"pickup_longitude\", \"pickup_latitude\", \n",
    "                        \"dropoff_datetime\", \"dropoff_longitude\", \"dropoff_latitude\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "boroughs = gpd.read_file(\"input/nyc-boroughs.geojson\")\n",
    "boroughs = boroughs.sort_values(by=\"boroughCode\", ascending=True)\n",
    "\n",
    "boroughs_list = [(row[\"borough\"], row[\"geometry\"]) for _, row in boroughs.iterrows()]\n",
    "\n",
    "sc = SparkSession.builder.getOrCreate().sparkContext\n",
    "boroughs_bc = sc.broadcast(boroughs_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import Point\n",
    "\n",
    "def get_borough(lat, lon):\n",
    "    \"\"\"Returns the borough name for given latitude and longitude.\"\"\"\n",
    "    if lat is None or lon is None:  \n",
    "        return \"Unknown\"\n",
    "    \n",
    "    point = Point(lon, lat)\n",
    "    \n",
    "    for borough_name, polygon in boroughs_bc.value:\n",
    "        if polygon.contains(point):\n",
    "            return borough_name\n",
    "    return \"Unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "get_borough_udf = udf(get_borough, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|dropoff_borough|\n",
      "+---------------+\n",
      "|Queens         |\n",
      "|Brooklyn       |\n",
      "|Staten Island  |\n",
      "|Manhattan      |\n",
      "|Bronx          |\n",
      "+---------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6373"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "df_selected = df_selected.withColumn(\"pickup_borough\", get_borough_udf(df_selected[\"pickup_latitude\"], df_selected[\"pickup_longitude\"]))\n",
    "df_selected = df_selected.withColumn(\"dropoff_borough\", get_borough_udf(df_selected[\"dropoff_latitude\"], df_selected[\"dropoff_longitude\"]))\n",
    "df_selected = df_selected.filter(col(\"dropoff_borough\") != \"Unknown\")\n",
    "df_selected.select(\"dropoff_borough\").distinct().show(truncate=False)\n",
    "\n",
    "df_selected.select(\"medallion\").distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QUERY 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import unix_timestamp, to_timestamp, col\n",
    "\n",
    "df_selected = df_selected.withColumn(\"pickup_datetime\", to_timestamp(col(\"pickup_datetime\"), \"dd-MM-yy HH:mm\"))\n",
    "df_selected = df_selected.withColumn(\"dropoff_datetime\", to_timestamp(col(\"dropoff_datetime\"), \"dd-MM-yy HH:mm\"))\n",
    "\n",
    "df_selected = df_selected.withColumn(\"pickup_ts\", unix_timestamp(col(\"pickup_datetime\")))\n",
    "df_selected = df_selected.withColumn(\"dropoff_ts\", unix_timestamp(col(\"dropoff_datetime\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6373"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, lag, sum as spark_sum, when\n",
    "\n",
    "df_selected = df_selected.withColumn(\"duration\", col(\"dropoff_ts\") - col(\"pickup_ts\"))\n",
    "df_selected = df_selected.select(\"medallion\", \"pickup_borough\", \"dropoff_borough\", \"pickup_ts\", \"dropoff_ts\", \"duration\")\n",
    "df_selected.select(\"medallion\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, lag, sum as spark_sum\n",
    "\n",
    "window_spec = Window.partitionBy(\"medallion\").orderBy(\"pickup_ts\")\n",
    "\n",
    "df_utilization = df_selected.withColumn(\"prev_dropoff\", lag(\"dropoff_ts\").over(window_spec))\n",
    "df_utilization = df_utilization.withColumn(\"idle_time\", when(\n",
    "    (col(\"prev_dropoff\").isNotNull()) & (col(\"pickup_ts\") - col(\"prev_dropoff\") <= 14400),\n",
    "    col(\"pickup_ts\") - col(\"prev_dropoff\")).otherwise(None))\n",
    "\n",
    "idle_time = df_utilization.groupBy(\"medallion\").agg(spark_sum(\"idle_time\").alias(\"total_idle_time\"))\n",
    "idle_time = idle_time.fillna(0, subset=[\"total_idle_time\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+-------------------+\n",
      "|           medallion|total_idle_time|total_occupied_time|\n",
      "+--------------------+---------------+-------------------+\n",
      "|0F621E366CFE63044...|          30600|              19020|\n",
      "|223670562219093D6...|          14580|               6120|\n",
      "|496036713FC662D71...|           8520|               3660|\n",
      "|4F4CA97166A04A455...|          21900|               9360|\n",
      "|59DF6039EC312EE6D...|          23040|              15900|\n",
      "|5CCB4924B158F945B...|          23520|              18780|\n",
      "|618BB39CEEAE5E9A6...|           8640|              12000|\n",
      "|6AFD7E44A278CFD00...|           8640|               3960|\n",
      "|72EAFBA3FB9F0507C...|          13800|              11580|\n",
      "|73039762E0F4B253E...|          23400|              14400|\n",
      "+--------------------+---------------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import sum as spark_sum, col\n",
    "\n",
    "window_spec = Window.partitionBy(\"medallion\")\n",
    "\n",
    "occupied_time = df_selected.groupBy(\"medallion\").agg(spark_sum(\"duration\").alias(\"total_occupied_time\"))\n",
    "occupied_time = occupied_time.fillna(0, subset=[\"total_occupied_time\"])\n",
    "idle_time = df_utilization.groupBy(\"medallion\").agg(spark_sum(\"idle_time\").alias(\"total_idle_time\"))\n",
    "\n",
    "df_final_utilization = occupied_time.join(idle_time, on=\"medallion\", how=\"inner\")\n",
    "\n",
    "df_final_utilization = df_final_utilization.cache()\n",
    "\n",
    "df_final_utilization.select(\"medallion\",'total_idle_time', \"total_occupied_time\").distinct().show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+---------------+----------+----------+--------+------------+---------+\n",
      "|           medallion|pickup_borough|dropoff_borough| pickup_ts|dropoff_ts|duration|prev_dropoff|idle_time|\n",
      "+--------------------+--------------+---------------+----------+----------+--------+------------+---------+\n",
      "|002E3B405B6ABEA23...|     Manhattan|      Manhattan|1358047920|1358048340|     420|        NULL|        0|\n",
      "|002E3B405B6ABEA23...|     Manhattan|      Manhattan|1358049900|1358051700|    1800|  1358048340|     1560|\n",
      "|002E3B405B6ABEA23...|     Manhattan|      Manhattan|1358052180|1358052720|     540|  1358051700|      480|\n",
      "|002E3B405B6ABEA23...|        Queens|      Manhattan|1358079060|1358080920|    1860|  1358052720|        0|\n",
      "|002E3B405B6ABEA23...|     Manhattan|      Manhattan|1358081940|1358082840|     900|  1358080920|     1020|\n",
      "|002E3B405B6ABEA23...|     Manhattan|      Manhattan|1358083320|1358084040|     720|  1358082840|      480|\n",
      "|002E3B405B6ABEA23...|     Manhattan|      Manhattan|1358084160|1358084520|     360|  1358084040|      120|\n",
      "|002E3B405B6ABEA23...|     Manhattan|      Manhattan|1358086620|1358087460|     840|  1358084520|     2100|\n",
      "|002E3B405B6ABEA23...|     Manhattan|      Manhattan|1358087520|1358088180|     660|  1358087460|       60|\n",
      "|002E3B405B6ABEA23...|     Manhattan|         Queens|1358092980|1358095080|    2100|  1358088180|     4800|\n",
      "+--------------------+--------------+---------------+----------+----------+--------+------------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_final_utilization = occupied_time.join(idle_time, on=\"medallion\", how=\"inner\")\n",
    "\n",
    "df_final_utilization = df_final_utilization.withColumn(\n",
    "    \"utilization\",\n",
    "    when(col(\"total_occupied_time\") + col(\"total_idle_time\") > 0,\n",
    "         col(\"total_occupied_time\") / (col(\"total_occupied_time\") + col(\"total_idle_time\"))\n",
    "    ).otherwise(0)\n",
    ")\n",
    "df_utilization = df_utilization.withColumn(\"idle_time\", when(col(\"idle_time\").isNull(), 0).otherwise(col(\"idle_time\")))\n",
    "df_utilization.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+---------------+-----------+\n",
      "|           medallion|total_occupied_time|total_idle_time|utilization|\n",
      "+--------------------+-------------------+---------------+-----------+\n",
      "|0F621E366CFE63044...|              19020|          30600|       0.38|\n",
      "|223670562219093D6...|               6120|          14580|        0.3|\n",
      "|496036713FC662D71...|               3660|           8520|        0.3|\n",
      "|4F4CA97166A04A455...|               9360|          21900|        0.3|\n",
      "|59DF6039EC312EE6D...|              15900|          23040|       0.41|\n",
      "|5CCB4924B158F945B...|              18780|          23520|       0.44|\n",
      "|618BB39CEEAE5E9A6...|              12000|           8640|       0.58|\n",
      "|6AFD7E44A278CFD00...|               3960|           8640|       0.31|\n",
      "|72EAFBA3FB9F0507C...|              11580|          13800|       0.46|\n",
      "|73039762E0F4B253E...|              14400|          23400|       0.38|\n",
      "+--------------------+-------------------+---------------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import round\n",
    "\n",
    "df_final_utilization = df_final_utilization.withColumn(\"utilization\", round(col(\"utilization\"), 2))\n",
    "df_final_utilization = df_final_utilization.filter(col(\"total_idle_time\").isNotNull())\n",
    "\n",
    "df_final_utilization.select(\"medallion\", \"total_occupied_time\", \"total_idle_time\", \"utilization\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QUERY 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Borough: Queens, Average Wait Time: 3054.28 seconds\n",
      "Borough: Brooklyn, Average Wait Time: 2634.88 seconds\n",
      "Borough: Staten Island, Average Wait Time: 4710.0 seconds\n",
      "Borough: Manhattan, Average Wait Time: 1047.16 seconds\n",
      "Borough: Bronx, Average Wait Time: 2575.73 seconds\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import lead, avg, col\n",
    "\n",
    "window_spec = Window.partitionBy(\"medallion\").orderBy(\"dropoff_ts\")\n",
    "df_selected = df_selected.withColumn(\"next_pickup_ts\", lead(\"pickup_ts\").over(window_spec))\n",
    "df_selected = df_selected.withColumn(\"wait_time\", col(\"next_pickup_ts\") - col(\"dropoff_ts\"))\n",
    "df_selected_wait = df_selected.withColumn(\"wait_time\", \n",
    "    when(col(\"wait_time\") <= 14400, col(\"wait_time\")).otherwise(None)\n",
    ")\n",
    "df_selected_wait = df_selected_wait.filter(col(\"wait_time\").isNotNull())\n",
    "df_selected_wait = df_selected_wait.filter(col(\"dropoff_borough\") != \"Unknown\")\n",
    "\n",
    "avg_wait_time = df_selected_wait.groupBy(\"dropoff_borough\").agg(\n",
    "    round(avg(\"wait_time\"), 2).alias(\"avg_wait_time\")\n",
    ")\n",
    "results = avg_wait_time.collect()\n",
    "\n",
    "for row in results:\n",
    "    print(f\"Borough: {row['dropoff_borough']}, Average Wait Time: {row['avg_wait_time']} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QUERY 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trips that started and ended in the same borough: 86074\n"
     ]
    }
   ],
   "source": [
    "same_borough_count = df_selected.filter(\n",
    "    (col(\"pickup_borough\") == col(\"dropoff_borough\")) &\n",
    "    (col(\"pickup_borough\") != \"Unknown\") & \n",
    "    (col(\"dropoff_borough\") != \"Unknown\")\n",
    ").count()\n",
    "\n",
    "print(f\"Total trips that started and ended in the same borough: {same_borough_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QUERY 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trips that started in one borough and ended in another: 11433\n"
     ]
    }
   ],
   "source": [
    "different_borough_count = df_selected.filter(\n",
    "    (col(\"pickup_borough\") != col(\"dropoff_borough\")) &\n",
    "    (col(\"pickup_borough\") != \"Unknown\") & \n",
    "    (col(\"dropoff_borough\") != \"Unknown\")\n",
    ").count()\n",
    "\n",
    "print(f\"Total trips that started in one borough and ended in another: {different_borough_count}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
