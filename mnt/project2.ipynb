{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bcbd7ef-536e-4e14-a1b6-b8676eae567c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Project 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49419ba6-c59c-4a68-ad70-47c27fabefc4",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14a96bab-99ba-47be-9638-a12d68f0e87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import unix_timestamp\n",
    "from pyspark.sql.functions import max, col, unix_timestamp, lit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f562cc-acec-4ed9-b16f-9228f5c9c350",
   "metadata": {},
   "source": [
    "### Query 0 - Data Cleansing and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3aa9bfc-c608-40c9-9511-5e2c08ce439d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Project\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2435c26d-88b0-4941-af68-99a8b28548ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_small = spark.read.csv(\"input/sorted_data_smaller.csv\", header=False, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "847a9407-5b54-4d93-b4a9-39269cee9af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\n",
    "    \"medallion\",\n",
    "    \"hack_license\",\n",
    "    \"pickup_datetime\",\n",
    "    \"dropoff_datetime\",\n",
    "    \"trip_time_in_secs\",\n",
    "    \"trip_distance\",\n",
    "    \"pickup_longitude\",\n",
    "    \"pickup_latitude\",\n",
    "    \"dropoff_longitude\",\n",
    "    \"dropoff_latitude\",\n",
    "    \"payment_type\",\n",
    "    \"fare_amount\",\n",
    "    \"surcharge\",\n",
    "    \"mta_tax\",\n",
    "    \"tip_amount\",\n",
    "    \"tolls_amount\",\n",
    "    \"total_amount\"\n",
    "]\n",
    "df_small = df_small.toDF(*columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bdac0318-8aa6-4a00-8afa-2ab6268cafa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------------------+-------------------+-----------------+-------------+----------------+---------------+-----------------+----------------+------------+-----------+---------+-------+----------+------------+------------+\n",
      "|           medallion|        hack_license|    pickup_datetime|   dropoff_datetime|trip_time_in_secs|trip_distance|pickup_longitude|pickup_latitude|dropoff_longitude|dropoff_latitude|payment_type|fare_amount|surcharge|mta_tax|tip_amount|tolls_amount|total_amount|\n",
      "+--------------------+--------------------+-------------------+-------------------+-----------------+-------------+----------------+---------------+-----------------+----------------+------------+-----------+---------+-------+----------+------------+------------+\n",
      "|5EE2C4D3BF57BDB45...|E96EF8F6E6122591F...|2013-01-01 00:00:09|2013-01-01 00:00:36|               26|          0.1|       -73.99221|      40.725124|       -73.991646|       40.726658|         CSH|        2.5|      0.5|    0.5|       0.0|         0.0|         3.5|\n",
      "|42730E78D8BE872B5...|6016A71F1D29D678E...|2013-01-01 00:01:00|2013-01-01 00:01:00|                0|         0.01|             0.0|            0.0|              0.0|             0.0|         CSH|        2.5|      0.5|    0.5|       0.0|         0.0|         3.5|\n",
      "|0CEBE42EAF42C3380...|CC7A4176549BA819E...|2013-01-01 00:00:00|2013-01-01 00:03:00|              180|         1.56|       -74.00975|      40.706432|       -73.971985|       40.794716|         CSH|        6.5|      0.5|    0.5|       0.0|         0.0|         7.5|\n",
      "|F07E8A597F1DF9BB3...|D0626B4EF37543B01...|2013-01-01 00:02:00|2013-01-01 00:03:00|               60|         0.77|      -73.944618|      40.783131|       -73.947418|       40.775574|         CSH|        4.5|      0.5|    0.5|       0.0|         0.0|         5.5|\n",
      "|6BA29E9A69B10F218...|ED368552102F12EA2...|2013-01-01 00:01:00|2013-01-01 00:04:00|              180|         0.74|      -73.971138|       40.75898|       -73.972206|       40.752502|         CRD|        4.5|      0.5|    0.5|       0.0|         0.0|         5.5|\n",
      "+--------------------+--------------------+-------------------+-------------------+-----------------+-------------+----------------+---------------+-----------------+----------------+------------+-----------+---------+-------+----------+------------+------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- medallion: string (nullable = true)\n",
      " |-- hack_license: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- trip_time_in_secs: integer (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- pickup_longitude: double (nullable = true)\n",
      " |-- pickup_latitude: double (nullable = true)\n",
      " |-- dropoff_longitude: double (nullable = true)\n",
      " |-- dropoff_latitude: double (nullable = true)\n",
      " |-- payment_type: string (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- surcharge: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_small.show(5)  # See the first 5 rows\n",
    "df_small.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5bddfde-a1de-444a-b5e4-5b65154c9752",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Remove rows with missing or 0.0 coordinates\n",
    "df_clean = df_small.filter(\n",
    "    (col(\"pickup_longitude\").isNotNull()) & (col(\"pickup_longitude\") != 0.0) &\n",
    "    (col(\"pickup_latitude\").isNotNull()) & (col(\"pickup_latitude\") != 0.0) &\n",
    "    (col(\"dropoff_longitude\").isNotNull()) & (col(\"dropoff_longitude\") != 0.0) &\n",
    "    (col(\"dropoff_latitude\").isNotNull()) & (col(\"dropoff_latitude\") != 0.0)\n",
    ")\n",
    "\n",
    "# Remove rows with missing medallions or licenses\n",
    "df_clean = df_clean.filter(\n",
    "    (col(\"medallion\").isNotNull()) & (col(\"medallion\") != \"\") &\n",
    "    (col(\"hack_license\").isNotNull()) & (col(\"hack_license\") != \"\")\n",
    ")\n",
    "# Tme Model - this supports time-based queries\n",
    "df_clean = df_clean.withColumn(\"pickup_ts\", unix_timestamp(\"pickup_datetime\")) \\\n",
    "                   .withColumn(\"dropoff_ts\", unix_timestamp(\"dropoff_datetime\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8067d3c4-b024-403a-ad10-4c6fc97a9dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original row count: 14432092\n",
      "Cleaned row count: 14186504\n"
     ]
    }
   ],
   "source": [
    "print(\"Original row count:\", df_small.count())\n",
    "print(\"Cleaned row count:\", df_clean.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bce39cf-d612-4697-a407-e3ce6dc00564",
   "metadata": {},
   "source": [
    "### Query 1: Frequent Routes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb4660c5-7088-4807-8f0e-8c1c5cd35150",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+---------------+\n",
      "|start_cell|end_cell|Number_of_Rides|\n",
      "+----------+--------+---------------+\n",
      "|155.160   |154.162 |2200           |\n",
      "|154.162   |155.160 |2042           |\n",
      "|156.159   |154.162 |2004           |\n",
      "|157.161   |154.162 |1935           |\n",
      "|154.162   |156.161 |1854           |\n",
      "|154.162   |156.159 |1763           |\n",
      "|154.162   |157.161 |1751           |\n",
      "|158.159   |157.161 |1711           |\n",
      "|155.160   |156.159 |1606           |\n",
      "|154.162   |155.164 |1578           |\n",
      "+----------+--------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from pyspark.sql.functions import col, udf, desc\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "def to_cell(lat, lon):\n",
    "    if not (40.5 <= lat <= 41.8 and -74.25 <= lon <= -73.7):\n",
    "        return None\n",
    "    lat0 = 41.474937\n",
    "    lon0 = -74.913585\n",
    "    meters_per_deg_lat = 111320\n",
    "    meters_per_deg_lon = 40075000 * math.cos(math.radians(lat0)) / 360\n",
    "    cell_x = int((lon - lon0) * meters_per_deg_lon / 500) + 1\n",
    "    cell_y = int((lat0 - lat) * meters_per_deg_lat / 500) + 1\n",
    "    if 1 <= cell_x <= 300 and 1 <= cell_y <= 300:\n",
    "        return f\"{cell_x}.{cell_y}\"\n",
    "    return None\n",
    "\n",
    "to_cell_udf = udf(to_cell, StringType())\n",
    "\n",
    "df_filtered = df_clean.withColumn(\"pickup_cell\", to_cell_udf(col(\"pickup_latitude\"), col(\"pickup_longitude\"))) \\\n",
    "                      .withColumn(\"dropoff_cell\", to_cell_udf(col(\"dropoff_latitude\"), col(\"dropoff_longitude\"))) \\\n",
    "                      .filter(col(\"pickup_cell\").isNotNull() & col(\"dropoff_cell\").isNotNull())\n",
    "\n",
    "routes = df_filtered.groupBy(\"pickup_cell\", \"dropoff_cell\").count()\n",
    "\n",
    "top10_routes = routes.orderBy(desc(\"count\")).limit(10)\n",
    "\n",
    "top10_routes.select(\n",
    "    col(\"pickup_cell\").alias(\"start_cell\"),\n",
    "    col(\"dropoff_cell\").alias(\"end_cell\"),\n",
    "    col(\"count\").alias(\"Number_of_Rides\")\n",
    ").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40598027-48bc-49bb-86d1-3c46061965ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "### PART 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e9a223ba-9221-47ce-b68a-ddc13fe2fb79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected sample into memory: 9997 rows\n",
      "\n",
      "Update #1\n",
      "Pickup: 2013-01-01 00:00:09\n",
      "Dropoff: 2013-01-01 00:00:36\n",
      "  Route 1: 154.167 → 154.167 — 1 rides\n",
      "  Route 2: None → None — None rides\n",
      "  Route 3: None → None — None rides\n",
      "  Route 4: None → None — None rides\n",
      "  Route 5: None → None — None rides\n",
      "  Route 6: None → None — None rides\n",
      "  Route 7: None → None — None rides\n",
      "  Route 8: None → None — None rides\n",
      "  Route 9: None → None — None rides\n",
      "  Route 10: None → None — None rides\n",
      "Delay: 385920091.993 seconds\n",
      "\n",
      "Update #2\n",
      "Pickup: 2013-01-01 00:00:00\n",
      "Dropoff: 2013-01-01 00:03:00\n",
      "  Route 1: 154.167 → 154.167 — 1 rides\n",
      "  Route 2: 151.172 → 158.152 — 1 rides\n",
      "  Route 3: None → None — None rides\n",
      "  Route 4: None → None — None rides\n",
      "  Route 5: None → None — None rides\n",
      "  Route 6: None → None — None rides\n",
      "  Route 7: None → None — None rides\n",
      "  Route 8: None → None — None rides\n",
      "  Route 9: None → None — None rides\n",
      "  Route 10: None → None — None rides\n",
      "Delay: 385919947.993 seconds\n",
      "\n",
      "Update #3\n",
      "Pickup: 2013-01-01 00:02:00\n",
      "Dropoff: 2013-01-01 00:03:00\n",
      "  Route 1: 154.167 → 154.167 — 1 rides\n",
      "  Route 2: 151.172 → 158.152 — 1 rides\n",
      "  Route 3: 162.155 → 162.156 — 1 rides\n",
      "  Route 4: None → None — None rides\n",
      "  Route 5: None → None — None rides\n",
      "  Route 6: None → None — None rides\n",
      "  Route 7: None → None — None rides\n",
      "  Route 8: None → None — None rides\n",
      "  Route 9: None → None — None rides\n",
      "  Route 10: None → None — None rides\n",
      "Delay: 385919947.993 seconds\n",
      "\n",
      "Update #4\n",
      "Pickup: 2013-01-01 00:01:00\n",
      "Dropoff: 2013-01-01 00:04:00\n",
      "  Route 1: 154.167 → 154.167 — 1 rides\n",
      "  Route 2: 151.172 → 158.152 — 1 rides\n",
      "  Route 3: 162.155 → 162.156 — 1 rides\n",
      "  Route 4: 158.160 → 158.161 — 1 rides\n",
      "  Route 5: None → None — None rides\n",
      "  Route 6: None → None — None rides\n",
      "  Route 7: None → None — None rides\n",
      "  Route 8: None → None — None rides\n",
      "  Route 9: None → None — None rides\n",
      "  Route 10: None → None — None rides\n",
      "Delay: 385919887.993 seconds\n",
      "\n",
      "Update #5\n",
      "Pickup: 2013-01-01 00:02:15\n",
      "Dropoff: 2013-01-01 00:04:01\n",
      "  Route 1: 154.167 → 154.167 — 1 rides\n",
      "  Route 2: 151.172 → 158.152 — 1 rides\n",
      "  Route 3: 162.155 → 162.156 — 1 rides\n",
      "  Route 4: 158.160 → 158.161 — 1 rides\n",
      "  Route 5: 154.160 → 153.159 — 1 rides\n",
      "  Route 6: None → None — None rides\n",
      "  Route 7: None → None — None rides\n",
      "  Route 8: None → None — None rides\n",
      "  Route 9: None → None — None rides\n",
      "  Route 10: None → None — None rides\n",
      "Delay: 385919886.993 seconds\n",
      "\n",
      "Done. Total updates triggered: 676\n",
      "\n",
      "Last 5 updates:\n",
      "\n",
      "Pickup: 2013-01-01 04:08:27, Dropoff: 2013-01-01 04:29:05\n",
      "  Route 1: 154.162 → 155.168 — 3 rides\n",
      "  Route 2: 156.166 → 161.156 — 2 rides\n",
      "  Route 3: 156.163 → 159.159 — 2 rides\n",
      "  Route 4: 157.163 → 157.162 — 2 rides\n",
      "  Route 5: 156.163 → 154.162 — 2 rides\n",
      "  Route 6: 153.163 → 154.159 — 2 rides\n",
      "  Route 7: 154.158 → 157.162 — 2 rides\n",
      "  Route 8: 157.162 → 155.164 — 2 rides\n",
      "  Route 9: 154.159 → 155.158 — 2 rides\n",
      "  Route 10: 154.164 → 154.168 — 2 rides\n",
      "Delay: 385903993.844 seconds\n",
      "\n",
      "Pickup: 2013-01-01 04:22:13, Dropoff: 2013-01-01 04:29:39\n",
      "  Route 1: 154.162 → 155.168 — 3 rides\n",
      "  Route 2: 156.163 → 159.159 — 2 rides\n",
      "  Route 3: 157.163 → 157.162 — 2 rides\n",
      "  Route 4: 156.163 → 154.162 — 2 rides\n",
      "  Route 5: 153.163 → 154.159 — 2 rides\n",
      "  Route 6: 154.158 → 157.162 — 2 rides\n",
      "  Route 7: 157.162 → 155.164 — 2 rides\n",
      "  Route 8: 154.159 → 155.158 — 2 rides\n",
      "  Route 9: 154.164 → 154.168 — 2 rides\n",
      "  Route 10: 154.163 → 157.162 — 2 rides\n",
      "Delay: 385903959.852 seconds\n",
      "\n",
      "Pickup: 2013-01-01 04:24:13, Dropoff: 2013-01-01 04:30:01\n",
      "  Route 1: 154.162 → 155.168 — 3 rides\n",
      "  Route 2: 156.163 → 154.162 — 2 rides\n",
      "  Route 3: 153.163 → 154.159 — 2 rides\n",
      "  Route 4: 154.158 → 157.162 — 2 rides\n",
      "  Route 5: 157.162 → 155.164 — 2 rides\n",
      "  Route 6: 154.159 → 155.158 — 2 rides\n",
      "  Route 7: 154.164 → 154.168 — 2 rides\n",
      "  Route 8: 154.163 → 157.162 — 2 rides\n",
      "  Route 9: 154.158 → 154.162 — 2 rides\n",
      "  Route 10: 154.157 → 157.161 — 2 rides\n",
      "Delay: 385903937.869 seconds\n",
      "\n",
      "Pickup: 2013-01-01 04:22:00, Dropoff: 2013-01-01 04:31:00\n",
      "  Route 1: 154.158 → 154.162 — 3 rides\n",
      "  Route 2: 154.162 → 155.168 — 3 rides\n",
      "  Route 3: 156.163 → 154.162 — 2 rides\n",
      "  Route 4: 153.163 → 154.159 — 2 rides\n",
      "  Route 5: 154.158 → 157.162 — 2 rides\n",
      "  Route 6: 157.162 → 155.164 — 2 rides\n",
      "  Route 7: 154.159 → 155.158 — 2 rides\n",
      "  Route 8: 154.164 → 154.168 — 2 rides\n",
      "  Route 9: 154.163 → 157.162 — 2 rides\n",
      "  Route 10: 154.157 → 157.161 — 2 rides\n",
      "Delay: 385903878.89 seconds\n",
      "\n",
      "Pickup: 2013-01-01 04:18:12, Dropoff: 2013-01-01 04:31:09\n",
      "  Route 1: 154.158 → 154.162 — 3 rides\n",
      "  Route 2: 154.162 → 155.168 — 3 rides\n",
      "  Route 3: 153.163 → 154.159 — 2 rides\n",
      "  Route 4: 154.158 → 157.162 — 2 rides\n",
      "  Route 5: 157.162 → 155.164 — 2 rides\n",
      "  Route 6: 154.159 → 155.158 — 2 rides\n",
      "  Route 7: 154.164 → 154.168 — 2 rides\n",
      "  Route 8: 154.163 → 157.162 — 2 rides\n",
      "  Route 9: 154.157 → 157.161 — 2 rides\n",
      "  Route 10: 155.169 → 155.161 — 2 rides\n",
      "Delay: 385903869.893 seconds\n"
     ]
    }
   ],
   "source": [
    "from datetime import timedelta\n",
    "import time\n",
    "\n",
    "# As the data isn't a stream meaning we don't have a Kafka or something set up, this is the next best thing: It starts sliding the window to imitate \n",
    "# the streaming data that is coming and updates the routes accordingly.\n",
    "\n",
    "# Uncomment this line to achieve longer run time with full data (i stopped at 23 minutes)\n",
    "# df_sample = df_clean\n",
    "\n",
    "\n",
    "df_sample = df_clean.limit(10000)\n",
    "\n",
    "df_pd = df_sample.withColumn(\"pickup_cell\", to_cell_udf(col(\"pickup_latitude\"), col(\"pickup_longitude\"))) \\\n",
    "                 .withColumn(\"dropoff_cell\", to_cell_udf(col(\"dropoff_latitude\"), col(\"dropoff_longitude\"))) \\\n",
    "                 .filter(col(\"pickup_cell\").isNotNull() & col(\"dropoff_cell\").isNotNull()) \\\n",
    "                 .orderBy(\"dropoff_datetime\") \\\n",
    "                 .select(\"pickup_datetime\", \"dropoff_datetime\", \"pickup_cell\", \"dropoff_cell\") \\\n",
    "                 .collect()\n",
    "\n",
    "print(\"Collected sample into memory:\", len(df_pd), \"rows\")\n",
    "\n",
    "window_minutes = 30\n",
    "results = []\n",
    "prev_top10_keys = []\n",
    "\n",
    "for i, row in enumerate(df_pd):\n",
    "    now = row[\"dropoff_datetime\"]\n",
    "    pickup_time = row[\"pickup_datetime\"]\n",
    "    window_start = now - timedelta(minutes=window_minutes)\n",
    "\n",
    "    j = i\n",
    "    while j >= 0 and df_pd[j][\"dropoff_datetime\"] >= window_start:\n",
    "        j -= 1\n",
    "    window_rows = df_pd[j+1:i+1]\n",
    "\n",
    "    route_counts = {}\n",
    "    for r in window_rows:\n",
    "        key = (r[\"pickup_cell\"], r[\"dropoff_cell\"])\n",
    "        route_counts[key] = route_counts.get(key, 0) + 1\n",
    "\n",
    "    top10 = sorted(route_counts.items(), key=lambda x: -x[1])[:10]\n",
    "    top10_keys = [route for route, _ in top10]\n",
    "\n",
    "    if top10_keys != prev_top10_keys:\n",
    "        prev_top10_keys = top10_keys.copy()\n",
    "\n",
    "        while len(top10) < 10:\n",
    "            top10.append(((None, None), None))\n",
    "\n",
    "        delay = round(time.time() - time.mktime(now.timetuple()), 3)\n",
    "\n",
    "        row_out = [pickup_time, now]\n",
    "        for (start, end), count in top10:\n",
    "            row_out.extend([start, end, count])\n",
    "        row_out.append(delay)\n",
    "        results.append(row_out)\n",
    "\n",
    "        if len(results) <= 5:\n",
    "            print(f\"\\nUpdate #{len(results)}\")\n",
    "            print(\"Pickup:\", pickup_time)\n",
    "            print(\"Dropoff:\", now)\n",
    "            for idx, ((s, e), c) in enumerate(top10, 1):\n",
    "                print(f\"  Route {idx}: {s} → {e} — {c} rides\")\n",
    "            print(\"Delay:\", delay, \"seconds\")\n",
    "\n",
    "print(\"\\nDone. Total updates triggered:\", len(results))\n",
    "\n",
    "if len(results) > 5:\n",
    "    print(\"\\nLast 5 updates:\")\n",
    "    for row in results[-5:]:\n",
    "        print(f\"\\nPickup: {row[0]}, Dropoff: {row[1]}\")\n",
    "        for i in range(2, 32, 3):\n",
    "            print(f\"  Route {(i - 2)//3 + 1}: {row[i]} → {row[i+1]} — {row[i+2]} rides\")\n",
    "        print(\"Delay:\", row[-1], \"seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3731e866",
   "metadata": {},
   "source": [
    "### Query 2: Profitable areas\n",
    "\n",
    "This query aims to identify areas that are currently most profitable for taxi drivers. The\n",
    "profitability of an area is determined by dividing the area's profit by the number of empty\n",
    "taxis in that area within the last 15 minutes. The profit originating from an area is computed\n",
    "by calculating the median fare + tip for trips that started in the area and ended within the last\n",
    "15 minutes. The number of empty taxis in an area is the sum of taxis with a drop-off location\n",
    "less than 30 minutes ago and no following pickup."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74016a46",
   "metadata": {},
   "source": [
    "#### Part 1\n",
    "The result stream of the query must be\n",
    "\n",
    "pickup_datetime, dropoff_datetime, profitable_cell_id_1,\n",
    "empty_taxies_in_cell_id, median_profit_in_cell_id, profitability_of_cell,\n",
    "\n",
    "Report only the 10 most profitable areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "caba7469-5b20-4f45-b628-6905065fbcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "def to_cell_250(lat, lon):\n",
    "    if lat is None or lon is None:\n",
    "        return None\n",
    "    lat0 = 41.474937\n",
    "    lon0 = -74.913585\n",
    "    meters_per_deg_lat = 111320\n",
    "    meters_per_deg_lon = 40075000 * math.cos(math.radians(lat0)) / 360\n",
    "    cell_x = int((lon - lon0) * meters_per_deg_lon / 250) + 1\n",
    "    cell_y = int((lat0 - lat) * meters_per_deg_lat / 250) + 1\n",
    "    if 1 <= cell_x <= 600 and 1 <= cell_y <= 600:\n",
    "        return f\"{cell_x}.{cell_y}\"\n",
    "    return None\n",
    "\n",
    "to_cell_250_udf = udf(to_cell_250, StringType())\n",
    "\n",
    "df_enriched = df_clean.withColumn(\"pickup_cell\", to_cell_250_udf(col(\"pickup_latitude\"), col(\"pickup_longitude\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a32f1d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------------------+-------------------+-----------------+-------------+----------------+---------------+-----------------+----------------+------------+-----------+---------+-------+----------+------------+------------+----------+----------+-----------+\n",
      "|           medallion|        hack_license|    pickup_datetime|   dropoff_datetime|trip_time_in_secs|trip_distance|pickup_longitude|pickup_latitude|dropoff_longitude|dropoff_latitude|payment_type|fare_amount|surcharge|mta_tax|tip_amount|tolls_amount|total_amount| pickup_ts|dropoff_ts|pickup_cell|\n",
      "+--------------------+--------------------+-------------------+-------------------+-----------------+-------------+----------------+---------------+-----------------+----------------+------------+-----------+---------+-------+----------+------------+------------+----------+----------+-----------+\n",
      "|5EE2C4D3BF57BDB45...|E96EF8F6E6122591F...|2013-01-01 00:00:09|2013-01-01 00:00:36|               26|          0.1|       -73.99221|      40.725124|       -73.991646|       40.726658|         CSH|        2.5|      0.5|    0.5|       0.0|         0.0|         3.5|1356991209|1356991236|    308.334|\n",
      "|0CEBE42EAF42C3380...|CC7A4176549BA819E...|2013-01-01 00:00:00|2013-01-01 00:03:00|              180|         1.56|       -74.00975|      40.706432|       -73.971985|       40.794716|         CSH|        6.5|      0.5|    0.5|       0.0|         0.0|         7.5|1356991200|1356991380|    302.343|\n",
      "|F07E8A597F1DF9BB3...|D0626B4EF37543B01...|2013-01-01 00:02:00|2013-01-01 00:03:00|               60|         0.77|      -73.944618|      40.783131|       -73.947418|       40.775574|         CSH|        4.5|      0.5|    0.5|       0.0|         0.0|         5.5|1356991320|1356991380|    324.309|\n",
      "|6BA29E9A69B10F218...|ED368552102F12EA2...|2013-01-01 00:01:00|2013-01-01 00:04:00|              180|         0.74|      -73.971138|       40.75898|       -73.972206|       40.752502|         CRD|        4.5|      0.5|    0.5|       0.0|         0.0|         5.5|1356991260|1356991440|    315.319|\n",
      "|46D8364DDA0DFC3B9...|A0DDBE3613E4163A3...|2013-01-01 00:02:15|2013-01-01 00:04:01|              105|          0.2|      -73.995094|      40.759735|        -73.99929|       40.761299|         CRD|        3.0|      0.5|    0.5|       0.0|         0.0|         4.0|1356991335|1356991441|    307.319|\n",
      "+--------------------+--------------------+-------------------+-------------------+-----------------+-------------+----------------+---------------+-----------------+----------------+------------+-----------+---------+-------+----------+------------+------------+----------+----------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_enriched.show(5)  # See the first 5 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec6bd82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_profit = df_enriched.withColumn(\"profit\", col(\"fare_amount\") + col(\"tip_amount\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7fe42bc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------------------+-------------------+-----------------+-------------+----------------+---------------+-----------------+----------------+------------+-----------+---------+-------+----------+------------+------------+----------+----------+-----------+------+\n",
      "|           medallion|        hack_license|    pickup_datetime|   dropoff_datetime|trip_time_in_secs|trip_distance|pickup_longitude|pickup_latitude|dropoff_longitude|dropoff_latitude|payment_type|fare_amount|surcharge|mta_tax|tip_amount|tolls_amount|total_amount| pickup_ts|dropoff_ts|pickup_cell|profit|\n",
      "+--------------------+--------------------+-------------------+-------------------+-----------------+-------------+----------------+---------------+-----------------+----------------+------------+-----------+---------+-------+----------+------------+------------+----------+----------+-----------+------+\n",
      "|5EE2C4D3BF57BDB45...|E96EF8F6E6122591F...|2013-01-01 00:00:09|2013-01-01 00:00:36|               26|          0.1|       -73.99221|      40.725124|       -73.991646|       40.726658|         CSH|        2.5|      0.5|    0.5|       0.0|         0.0|         3.5|1356991209|1356991236|    308.334|   2.5|\n",
      "|0CEBE42EAF42C3380...|CC7A4176549BA819E...|2013-01-01 00:00:00|2013-01-01 00:03:00|              180|         1.56|       -74.00975|      40.706432|       -73.971985|       40.794716|         CSH|        6.5|      0.5|    0.5|       0.0|         0.0|         7.5|1356991200|1356991380|    302.343|   6.5|\n",
      "|F07E8A597F1DF9BB3...|D0626B4EF37543B01...|2013-01-01 00:02:00|2013-01-01 00:03:00|               60|         0.77|      -73.944618|      40.783131|       -73.947418|       40.775574|         CSH|        4.5|      0.5|    0.5|       0.0|         0.0|         5.5|1356991320|1356991380|    324.309|   4.5|\n",
      "|6BA29E9A69B10F218...|ED368552102F12EA2...|2013-01-01 00:01:00|2013-01-01 00:04:00|              180|         0.74|      -73.971138|       40.75898|       -73.972206|       40.752502|         CRD|        4.5|      0.5|    0.5|       0.0|         0.0|         5.5|1356991260|1356991440|    315.319|   4.5|\n",
      "|46D8364DDA0DFC3B9...|A0DDBE3613E4163A3...|2013-01-01 00:02:15|2013-01-01 00:04:01|              105|          0.2|      -73.995094|      40.759735|        -73.99929|       40.761299|         CRD|        3.0|      0.5|    0.5|       0.0|         0.0|         4.0|1356991335|1356991441|    307.319|   3.0|\n",
      "+--------------------+--------------------+-------------------+-------------------+-----------------+-------------+----------------+---------------+-----------------+----------------+------------+-----------+---------+-------+----------+------------+------------+----------+----------+-----------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_profit.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca191f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "import time\n",
    "from pyspark.sql.functions import expr\n",
    "from pyspark.sql.functions import desc\n",
    "\n",
    "# Collect a sample of rows ordered by dropoff_ts to simulate event-time progression\n",
    "df_sample = df_profit.limit(10000) # lets take all the rows from the data since we already have a subset of the whole data.\n",
    "rows = df_sample.orderBy(\"dropoff_ts\").select(\"pickup_datetime\", \"dropoff_datetime\", \"dropoff_ts\", \"pickup_cell\", \"profit\").collect()\n",
    "\n",
    "results = []\n",
    "prev_top10_keys = []\n",
    "\n",
    "for i, row in enumerate(rows):\n",
    "    # Use each row's drop-off time as the current event time (in Unix seconds)\n",
    "    current_event_ts = row[\"dropoff_ts\"]\n",
    "    profit_window_start = current_event_ts - 15 * 60  # last 15 minutes\n",
    "    empty_window_start = current_event_ts - 30 * 60   # last 30 minutes\n",
    "\n",
    "    # Filter data within the respective windows\n",
    "    df_profit_window = df_profit.filter(col(\"dropoff_ts\") >= profit_window_start)\n",
    "    df_empty_window = df_enriched.filter(col(\"dropoff_ts\") >= empty_window_start)\n",
    "    \n",
    "    # Aggregate median profit per cell in the profit window\n",
    "    median_profit_df = df_profit_window.groupBy(\"pickup_cell\") \\\n",
    "        .agg(expr(\"percentile_approx(profit, 0.5)\").alias(\"median_profit\"))\n",
    "    \n",
    "    # Aggregate empty taxi counts per cell in the empty window (using dropoff_cell if available,\n",
    "    # or you may assume pickup_cell==dropoff_cell for simplicity)\n",
    "    # Here, we assume the same cell function applies for drop-off coordinates.\n",
    "    df_empty_window = df_empty_window.withColumn(\"dropoff_cell\", to_cell_250_udf(col(\"dropoff_latitude\"), col(\"dropoff_longitude\")))\n",
    "    from pyspark.sql.functions import countDistinct\n",
    "    empty_taxis_df = df_empty_window.groupBy(\"dropoff_cell\") \\\n",
    "        .agg(countDistinct(\"medallion\").alias(\"empty_taxis\"))\n",
    "    \n",
    "    # Join on cell (assuming pickup_cell matches dropoff_cell for profit calculation)\n",
    "    # what do we do if the cells dont match up?\n",
    "    # what do we do if the pickup_cell is not the dropoff_cell? it way well not be the case?\n",
    "    df_profitability = median_profit_df.join(\n",
    "        empty_taxis_df,\n",
    "        median_profit_df.pickup_cell == empty_taxis_df.dropoff_cell,\n",
    "        \"inner\"\n",
    "    ).select(\n",
    "        median_profit_df.pickup_cell.alias(\"cell_id\"),\n",
    "        \"median_profit\",\n",
    "        \"empty_taxis\"\n",
    "    )\n",
    "    \n",
    "    # Compute profitability for each cell\n",
    "    # i guess this is based on the equation that was given in the original task description\n",
    "    df_profitability = df_profitability.withColumn(\"profitability\", col(\"median_profit\") / col(\"empty_taxis\"))\n",
    "    \n",
    "    # Get the top 10 cells by profitability for this event\n",
    "    top10_cells = df_profitability.orderBy(desc(\"profitability\")).limit(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e48f4b8",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o550257.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 9 in stage 25.0 failed 1 times, most recent failure: Lost task 9.0 in stage 25.0 (TID 188) (10.10.53.106 executor driver): java.net.SocketException: Connection reset by peer: socket write error\r\n\tat java.net.SocketOutputStream.socketWrite0(Native Method)\r\n\tat java.net.SocketOutputStream.socketWrite(Unknown Source)\r\n\tat java.net.SocketOutputStream.write(Unknown Source)\r\n\tat java.io.BufferedOutputStream.flushBuffer(Unknown Source)\r\n\tat java.io.BufferedOutputStream.write(Unknown Source)\r\n\tat java.io.DataOutputStream.write(Unknown Source)\r\n\tat java.io.FilterOutputStream.write(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:310)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:322)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:322)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$PythonUDFWriterThread.writeIteratorToStream(PythonUDFRunner.scala:58)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: java.net.SocketException: Connection reset by peer: socket write error\r\n\tat java.net.SocketOutputStream.socketWrite0(Native Method)\r\n\tat java.net.SocketOutputStream.socketWrite(Unknown Source)\r\n\tat java.net.SocketOutputStream.write(Unknown Source)\r\n\tat java.io.BufferedOutputStream.flushBuffer(Unknown Source)\r\n\tat java.io.BufferedOutputStream.write(Unknown Source)\r\n\tat java.io.DataOutputStream.write(Unknown Source)\r\n\tat java.io.FilterOutputStream.write(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:310)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:322)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:322)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$PythonUDFWriterThread.writeIteratorToStream(PythonUDFRunner.scala:58)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtop10_cells\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCell: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcell_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Profitability: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprofitability\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\anderoraava\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\dataframe.py:1263\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1243\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns all the records as a list of :class:`Row`.\u001b[39;00m\n\u001b[0;32m   1244\u001b[0m \n\u001b[0;32m   1245\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1260\u001b[0m \u001b[38;5;124;03m[Row(age=14, name='Tom'), Row(age=23, name='Alice'), Row(age=16, name='Bob')]\u001b[39;00m\n\u001b[0;32m   1261\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1262\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sc):\n\u001b[1;32m-> 1263\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectToPython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1264\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n",
      "File \u001b[1;32mc:\\Users\\anderoraava\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\anderoraava\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mc:\\Users\\anderoraava\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o550257.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 9 in stage 25.0 failed 1 times, most recent failure: Lost task 9.0 in stage 25.0 (TID 188) (10.10.53.106 executor driver): java.net.SocketException: Connection reset by peer: socket write error\r\n\tat java.net.SocketOutputStream.socketWrite0(Native Method)\r\n\tat java.net.SocketOutputStream.socketWrite(Unknown Source)\r\n\tat java.net.SocketOutputStream.write(Unknown Source)\r\n\tat java.io.BufferedOutputStream.flushBuffer(Unknown Source)\r\n\tat java.io.BufferedOutputStream.write(Unknown Source)\r\n\tat java.io.DataOutputStream.write(Unknown Source)\r\n\tat java.io.FilterOutputStream.write(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:310)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:322)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:322)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$PythonUDFWriterThread.writeIteratorToStream(PythonUDFRunner.scala:58)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: java.net.SocketException: Connection reset by peer: socket write error\r\n\tat java.net.SocketOutputStream.socketWrite0(Native Method)\r\n\tat java.net.SocketOutputStream.socketWrite(Unknown Source)\r\n\tat java.net.SocketOutputStream.write(Unknown Source)\r\n\tat java.io.BufferedOutputStream.flushBuffer(Unknown Source)\r\n\tat java.io.BufferedOutputStream.write(Unknown Source)\r\n\tat java.io.DataOutputStream.write(Unknown Source)\r\n\tat java.io.FilterOutputStream.write(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:310)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:322)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:322)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$PythonUDFWriterThread.writeIteratorToStream(PythonUDFRunner.scala:58)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\r\n"
     ]
    }
   ],
   "source": [
    "top10_cells.limit(10).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65ca7db",
   "metadata": {},
   "source": [
    "#### Part 2\n",
    "\n",
    "The resulting stream of the query must provide the 10 most profitable areas in the subsequent\n",
    "format:\n",
    "\n",
    "pickup_datetime, dropoff_datetime, profitable_cell_id_1,\n",
    "empty_taxies_in_cell_id_1, median_profit_in_cell_id_1,\n",
    "profitability_of_cell_1, ... , profitable_cell_id_10,\n",
    "empty_taxies_in_cell_id_10, median_profit_in_cell_id_10,\n",
    "profitability_of_cell_10, delay\n",
    "\n",
    "With attribute names containing cell_id_1 corresponding to the most profitable cell and\n",
    "attribute containing cell_id_10 corresponding to the 10th most profitable cell. If less than 10\n",
    "cells can be identified within the last 30 min, then NULL is to be returned for all cells that\n",
    "lack data. Query results must be updated whenever the 10 most profitable areas change. The\n",
    "pickup_datetime dropoff_datetime in the output are the timestamps of the trip report that\n",
    "triggered the change.\n",
    "\n",
    "The attribute “delay” captures the time delay between reading the input event that triggered\n",
    "the output and the time when the output is produced. Participants must determine the delay\n",
    "using the current system time right after reading the input and right before writing the output.\n",
    "This attribute will be used in the evaluation of the submission.\n",
    "\n",
    "Note: We use the same numbering scheme as for query 1 but with a different resolution. In\n",
    "query two we assume a cell size of 250m X 250m, i.e., the area to be considered spans from\n",
    "cell 1.1 to cell 600.600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc91816-02fe-445a-a5d9-048ca74300f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "import time\n",
    "from pyspark.sql.functions import expr, desc, col, countDistinct\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# Sample limited data (already a subset), simulate event-time progression\n",
    "df_sample = df_profit.limit(10000)\n",
    "rows = df_sample.orderBy(\"dropoff_ts\").select(\"pickup_datetime\", \"dropoff_datetime\", \"dropoff_ts\", \"pickup_cell\", \"profit\").collect()\n",
    "\n",
    "results = []\n",
    "prev_top10_keys = []\n",
    "\n",
    "for i, row in enumerate(rows):\n",
    "    current_event_ts = row[\"dropoff_ts\"]\n",
    "    profit_window_start = current_event_ts - 15 * 60  # last 15 min\n",
    "    empty_window_start = current_event_ts - 30 * 60   # last 30 min\n",
    "\n",
    "    # Profit window: calculate median profit per pickup cell\n",
    "    df_profit_window = df_profit.filter(col(\"dropoff_ts\") >= profit_window_start)\n",
    "    median_profit_df = df_profit_window.groupBy(\"pickup_cell\") \\\n",
    "        .agg(expr(\"percentile_approx(profit, 0.5)\").alias(\"median_profit\"))\n",
    "\n",
    "    # Empty taxi window: count distinct medallions per dropoff cell\n",
    "    df_empty_window = df_enriched.filter(col(\"dropoff_ts\") >= empty_window_start) \\\n",
    "        .withColumn(\"dropoff_cell\", to_cell_250_udf(col(\"dropoff_latitude\"), col(\"dropoff_longitude\")))\n",
    "\n",
    "    empty_taxis_df = df_empty_window.groupBy(\"dropoff_cell\") \\\n",
    "        .agg(countDistinct(\"medallion\").alias(\"empty_taxis\"))\n",
    "\n",
    "    # Join profit and empty counts on matching cells (pickup_cell == dropoff_cell)\n",
    "    df_profitability = median_profit_df.join(\n",
    "        empty_taxis_df,\n",
    "        median_profit_df.pickup_cell == empty_taxis_df.dropoff_cell,\n",
    "        \"inner\"\n",
    "    ).select(\n",
    "        median_profit_df.pickup_cell.alias(\"cell_id\"),\n",
    "        \"median_profit\",\n",
    "        \"empty_taxis\"\n",
    "    ).withColumn(\n",
    "        \"profitability\", col(\"median_profit\") / col(\"empty_taxis\")\n",
    "    )\n",
    "\n",
    "    # Get top 10 profitable cells\n",
    "    top10_cells = df_profitability.orderBy(desc(\"profitability\")).limit(10)\n",
    "    top10_list = top10_cells.collect()\n",
    "\n",
    "    # Fill in remaining slots with None if fewer than 10 found\n",
    "    while len(top10_list) < 10:\n",
    "        top10_list.append(Row(cell_id=None, median_profit=None, empty_taxis=None, profitability=None))\n",
    "\n",
    "    top10_keys = [cell.cell_id for cell in top10_list]\n",
    "\n",
    "    if top10_keys != prev_top10_keys:\n",
    "        prev_top10_keys = top10_keys.copy()\n",
    "        delay = round(time.time() - current_event_ts, 3)\n",
    "\n",
    "        result_row = {\n",
    "            \"pickup_datetime\": row[\"pickup_datetime\"],\n",
    "            \"dropoff_datetime\": row[\"dropoff_datetime\"]\n",
    "        }\n",
    "\n",
    "        for idx, cell in enumerate(top10_list, 1):\n",
    "            result_row[f\"profitable_cell_id_{idx}\"] = cell.cell_id\n",
    "            result_row[f\"empty_taxies_in_cell_id_{idx}\"] = cell.empty_taxis\n",
    "            result_row[f\"median_profit_in_cell_id_{idx}\"] = cell.median_profit\n",
    "            result_row[f\"profitability_of_cell_{idx}\"] = cell.profitability\n",
    "\n",
    "        result_row[\"delay\"] = delay\n",
    "        results.append(result_row)\n",
    "\n",
    "        # Display first few updates only for demonstration\n",
    "        if len(results) <= 3:\n",
    "            print(f\"\\nUpdate #{len(results)}\")\n",
    "            for key, value in result_row.items():\n",
    "                print(f\"{key}: {value}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
