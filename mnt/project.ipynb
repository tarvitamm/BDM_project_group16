{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nupyter Jotebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Ingestion and Combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load NYC taxi data\n",
    "df = pd.read_csv(\"input/sample_nyc_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark\n",
    "spark = SparkSession.builder.appName(\"NYC Taxi Analysis\").getOrCreate()\n",
    "\n",
    "# Load data into PySpark DataFrame\n",
    "df_spark = spark.read.csv(\"input/sample_nyc_data.csv\", header=True, inferSchema=True)\n",
    "\n",
    "df_selected = df_spark.select(\"medallion\", \n",
    "                        \"pickup_datetime\", \"pickup_longitude\", \"pickup_latitude\", \n",
    "                        \"dropoff_datetime\", \"dropoff_longitude\", \"dropoff_latitude\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>borough</th>\n",
       "      <th>boroughCode</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>Manhattan</td>\n",
       "      <td>1</td>\n",
       "      <td>POLYGON ((-74.01675 40.69334, -74.0154 40.6930...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>Manhattan</td>\n",
       "      <td>1</td>\n",
       "      <td>POLYGON ((-73.92641 40.87762, -73.9263 40.8774...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>Manhattan</td>\n",
       "      <td>1</td>\n",
       "      <td>POLYGON ((-73.92134 40.80085, -73.92031 40.799...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>Manhattan</td>\n",
       "      <td>1</td>\n",
       "      <td>POLYGON ((-73.93805 40.78083, -73.93779 40.780...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>Manhattan</td>\n",
       "      <td>1</td>\n",
       "      <td>POLYGON ((-73.9418 40.76905, -73.94286 40.7683...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      borough  boroughCode                                           geometry\n",
       "51  Manhattan            1  POLYGON ((-74.01675 40.69334, -74.0154 40.6930...\n",
       "72  Manhattan            1  POLYGON ((-73.92641 40.87762, -73.9263 40.8774...\n",
       "71  Manhattan            1  POLYGON ((-73.92134 40.80085, -73.92031 40.799...\n",
       "70  Manhattan            1  POLYGON ((-73.93805 40.78083, -73.93779 40.780...\n",
       "69  Manhattan            1  POLYGON ((-73.9418 40.76905, -73.94286 40.7683..."
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "# Load borough boundaries from GeoJSON\n",
    "boroughs = gpd.read_file(\"input/nyc-boroughs.geojson\")\n",
    "boroughs = boroughs.sort_values(by=\"boroughCode\", ascending=True)\n",
    "\n",
    "# Check sorted data\n",
    "boroughs[[\"borough\", \"boroughCode\", \"geometry\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import Point\n",
    "\n",
    "def get_borough(lat, lon):\n",
    "    \"\"\"Returns the borough name for a given latitude and longitude.\"\"\"\n",
    "    point = Point(lon, lat)\n",
    "    for _, row in boroughs.iterrows():\n",
    "        if row['geometry'].contains(point):\n",
    "            return row['borough']\n",
    "    return \"Unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Convert function into a Spark UDF\n",
    "get_borough_udf = udf(get_borough, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|dropoff_borough|\n",
      "+---------------+\n",
      "|Queens         |\n",
      "|Unknown        |\n",
      "|Brooklyn       |\n",
      "|Staten Island  |\n",
      "|Manhattan      |\n",
      "|Bronx          |\n",
      "+---------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6444"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_selected = df_selected.withColumn(\"pickup_borough\", get_borough_udf(df_selected[\"pickup_latitude\"], df_selected[\"pickup_longitude\"]))\n",
    "df_selected = df_selected.withColumn(\"dropoff_borough\", get_borough_udf(df_selected[\"dropoff_latitude\"], df_selected[\"dropoff_longitude\"]))\n",
    "df_selected.select(\"dropoff_borough\").distinct().show(truncate=False)\n",
    "\n",
    "df_selected.select(\"medallion\").distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QUERY 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|pickup_datetime|\n",
      "+---------------+\n",
      "|01-01-13 15:11 |\n",
      "|06-01-13 00:18 |\n",
      "|05-01-13 18:49 |\n",
      "|07-01-13 23:54 |\n",
      "|07-01-13 23:25 |\n",
      "|07-01-13 15:27 |\n",
      "|08-01-13 11:01 |\n",
      "|07-01-13 12:39 |\n",
      "|07-01-13 18:15 |\n",
      "|07-01-13 15:33 |\n",
      "+---------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "root\n",
      " |-- medallion: string (nullable = true)\n",
      " |-- pickup_datetime: string (nullable = true)\n",
      " |-- pickup_longitude: double (nullable = true)\n",
      " |-- pickup_latitude: double (nullable = true)\n",
      " |-- dropoff_datetime: string (nullable = true)\n",
      " |-- dropoff_longitude: double (nullable = true)\n",
      " |-- dropoff_latitude: double (nullable = true)\n",
      " |-- pickup_borough: string (nullable = true)\n",
      " |-- dropoff_borough: string (nullable = true)\n",
      "\n",
      "+---------------+\n",
      "|pickup_datetime|\n",
      "+---------------+\n",
      "|13-01-13 04:09 |\n",
      "|13-01-13 11:30 |\n",
      "|13-01-13 10:44 |\n",
      "|13-01-13 10:42 |\n",
      "|13-01-13 09:35 |\n",
      "|13-01-13 08:21 |\n",
      "|13-01-13 04:55 |\n",
      "|13-01-13 03:37 |\n",
      "|13-01-13 00:49 |\n",
      "|13-01-13 03:16 |\n",
      "|13-01-13 12:47 |\n",
      "|10-01-13 15:34 |\n",
      "|13-01-13 05:24 |\n",
      "|13-01-13 08:17 |\n",
      "|13-01-13 12:14 |\n",
      "|13-01-13 10:41 |\n",
      "|13-01-13 13:34 |\n",
      "|13-01-13 04:11 |\n",
      "|13-01-13 03:26 |\n",
      "|13-01-13 08:20 |\n",
      "+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_selected.select(\"pickup_datetime\").show(10, truncate=False)\n",
    "df_selected.printSchema()\n",
    "\n",
    "df_selected.select(\"pickup_datetime\").distinct().show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+-------------------+----------+\n",
      "|pickup_datetime    |pickup_ts |dropoff_datetime   |dropoff_ts|\n",
      "+-------------------+----------+-------------------+----------+\n",
      "|2013-01-01 15:11:00|1357053060|2013-01-01 15:18:00|1357053480|\n",
      "|2013-01-06 00:18:00|1357431480|2013-01-06 00:22:00|1357431720|\n",
      "|2013-01-05 18:49:00|1357411740|2013-01-05 18:54:00|1357412040|\n",
      "|2013-01-07 23:54:00|1357602840|2013-01-07 23:58:00|1357603080|\n",
      "|2013-01-07 23:25:00|1357601100|2013-01-07 23:34:00|1357601640|\n",
      "|2013-01-07 15:27:00|1357572420|2013-01-07 15:38:00|1357573080|\n",
      "|2013-01-08 11:01:00|1357642860|2013-01-08 11:08:00|1357643280|\n",
      "|2013-01-07 12:39:00|1357562340|2013-01-07 13:10:00|1357564200|\n",
      "|2013-01-07 18:15:00|1357582500|2013-01-07 18:20:00|1357582800|\n",
      "|2013-01-07 15:33:00|1357572780|2013-01-07 15:49:00|1357573740|\n",
      "+-------------------+----------+-------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import unix_timestamp, to_timestamp, col\n",
    "\n",
    "# Convert to TimestampType first\n",
    "df_selected = df_selected.withColumn(\"pickup_datetime\", to_timestamp(col(\"pickup_datetime\"), \"dd-MM-yy HH:mm\"))\n",
    "df_selected = df_selected.withColumn(\"dropoff_datetime\", to_timestamp(col(\"dropoff_datetime\"), \"dd-MM-yy HH:mm\"))\n",
    "\n",
    "# Convert to Unix epoch time (milliseconds)\n",
    "df_selected = df_selected.withColumn(\"pickup_ts\", unix_timestamp(col(\"pickup_datetime\")))\n",
    "df_selected = df_selected.withColumn(\"dropoff_ts\", unix_timestamp(col(\"dropoff_datetime\")))\n",
    "\n",
    "# Show results\n",
    "df_selected.select(\"pickup_datetime\", \"pickup_ts\", \"dropoff_datetime\", \"dropoff_ts\").show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6435"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Compute trip duration in seconds\n",
    "df_selected = df_selected.withColumn(\"duration\", col(\"dropoff_ts\") - col(\"pickup_ts\"))\n",
    "df_selected = df_selected.filter((col(\"duration\") > 0) & (col(\"duration\") <= 14400))\n",
    "df_selected = df_selected.select(\"medallion\", \"pickup_borough\", \"dropoff_borough\", \"pickup_ts\", \"dropoff_ts\", \"duration\")\n",
    "\n",
    "# Show final dataset\n",
    "df_selected.select(\"medallion\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6435"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, lag, sum as spark_sum\n",
    "\n",
    "# Define window partitioned by taxi (medallion) and ordered by pickup time\n",
    "window_spec = Window.partitionBy(\"medallion\").orderBy(\"pickup_ts\")\n",
    "\n",
    "# Get previous trip's drop-off time\n",
    "df_previous_trip = df_selected.withColumn(\"prev_dropoff\", lag(\"dropoff_ts\").over(window_spec))\n",
    "\n",
    "# Compute idle time (time between last drop-off and next pickup)\n",
    "df_previous_trip = df_previous_trip.withColumn(\"idle_time\", col(\"pickup_ts\") - col(\"prev_dropoff\"))\n",
    "df_previous_trip = df_previous_trip.filter(col(\"idle_time\").isNotNull() & (col(\"idle_time\") <= 14400))\n",
    "idle_time = df_previous_trip.groupBy(\"medallion\").agg(spark_sum(\"idle_time\").alias(\"total_idle_time\"))\n",
    "df_selected.select(\"medallion\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+\n",
      "|           medallion|total_occupied_time|\n",
      "+--------------------+-------------------+\n",
      "|000318C2E3E638158...|              13920|\n",
      "|002B4CFC5B8920A87...|              11100|\n",
      "|002E3B405B6ABEA23...|              10260|\n",
      "|0030AD2648D81EE87...|               1980|\n",
      "|0035520A854E4F276...|               8700|\n",
      "|0036961468659D0BF...|              11700|\n",
      "|003889E315BFDD985...|               4740|\n",
      "|0038EF45118925A51...|              10920|\n",
      "|003D87DB553C6F00F...|              12780|\n",
      "|003EEA559FA618008...|              14580|\n",
      "+--------------------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6435"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import sum as spark_sum, col\n",
    "\n",
    "# Define a window for each taxi\n",
    "window_spec = Window.partitionBy(\"medallion\")\n",
    "\n",
    "# Compute total occupied time (sum of trip durations per taxi)\n",
    "df_selected_sum = df_selected.withColumn(\"total_occupied_time\", spark_sum(\"duration\").over(window_spec))\n",
    "\n",
    "df_selected_sum.select(\"medallion\", \"total_occupied_time\").distinct().show(10)\n",
    "df_selected.select(\"medallion\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+---------------+\n",
      "|           medallion|total_occupied_time|total_idle_time|\n",
      "+--------------------+-------------------+---------------+\n",
      "|000318C2E3E638158...|              13920|          17400|\n",
      "|002B4CFC5B8920A87...|              11100|          17700|\n",
      "|002E3B405B6ABEA23...|              10260|          16140|\n",
      "|0030AD2648D81EE87...|               1980|            720|\n",
      "|0035520A854E4F276...|               8700|          14880|\n",
      "|0036961468659D0BF...|              11700|          19740|\n",
      "|003889E315BFDD985...|               4740|           9480|\n",
      "|0038EF45118925A51...|              10920|          15120|\n",
      "|003D87DB553C6F00F...|              12780|          12180|\n",
      "|003EEA559FA618008...|              14580|          38640|\n",
      "+--------------------+-------------------+---------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_utilization = df_selected_sum.select(\"medallion\", \"total_occupied_time\").distinct().join(\n",
    "    idle_time, on=\"medallion\", how=\"inner\"\n",
    ")\n",
    "\n",
    "# Show merged data\n",
    "df_utilization.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+---------------+-----------+\n",
      "|           medallion|total_occupied_time|total_idle_time|utilization|\n",
      "+--------------------+-------------------+---------------+-----------+\n",
      "|000318C2E3E638158...|              13920|          17400|       0.44|\n",
      "|002B4CFC5B8920A87...|              11100|          17700|       0.39|\n",
      "|002E3B405B6ABEA23...|              10260|          16140|       0.39|\n",
      "|0030AD2648D81EE87...|               1980|            720|       0.73|\n",
      "|0035520A854E4F276...|               8700|          14880|       0.37|\n",
      "|0036961468659D0BF...|              11700|          19740|       0.37|\n",
      "|003889E315BFDD985...|               4740|           9480|       0.33|\n",
      "|0038EF45118925A51...|              10920|          15120|       0.42|\n",
      "|003D87DB553C6F00F...|              12780|          12180|       0.51|\n",
      "|003EEA559FA618008...|              14580|          38640|       0.27|\n",
      "+--------------------+-------------------+---------------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import round\n",
    "\n",
    "# Compute utilization and round to 2 decimal places\n",
    "df_utilization = df_utilization.withColumn(\n",
    "    \"utilization\", round(col(\"total_occupied_time\") / (col(\"total_occupied_time\") + col(\"total_idle_time\")), 2)\n",
    ")\n",
    "\n",
    "# Show utilization percentage rounded to 2 decimal places\n",
    "df_utilization.select(\"medallion\", \"total_occupied_time\", \"total_idle_time\", \"utilization\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QUERY 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+----------+--------------+---------+\n",
      "|           medallion|dropoff_borough|dropoff_ts|next_pickup_ts|wait_time|\n",
      "+--------------------+---------------+----------+--------------+---------+\n",
      "|000318C2E3E638158...|      Manhattan|1358052900|    1358070300|    17400|\n",
      "|000318C2E3E638158...|        Unknown|1358070420|    1358070960|      540|\n",
      "|000318C2E3E638158...|        Unknown|1358071440|    1358071500|       60|\n",
      "|000318C2E3E638158...|        Unknown|1358071860|    1358073060|     1200|\n",
      "|000318C2E3E638158...|        Unknown|1358073660|    1358074020|      360|\n",
      "|000318C2E3E638158...|        Unknown|1358074320|    1358074560|      240|\n",
      "|000318C2E3E638158...|        Unknown|1358075040|    1358075400|      360|\n",
      "|000318C2E3E638158...|        Unknown|1358076420|    1358077020|      600|\n",
      "|000318C2E3E638158...|        Unknown|1358077380|    1358077920|      540|\n",
      "|000318C2E3E638158...|        Unknown|1358078280|    1358079060|      780|\n",
      "+--------------------+---------------+----------+--------------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import lead, avg, col\n",
    "\n",
    "# Define window partitioned by taxi (medallion) and ordered by drop-off time\n",
    "window_spec = Window.partitionBy(\"medallion\").orderBy(\"dropoff_ts\")\n",
    "\n",
    "# Get the next trip's pickup time\n",
    "df_selected = df_selected.withColumn(\"next_pickup_ts\", lead(\"pickup_ts\").over(window_spec))\n",
    "\n",
    "# Compute the time difference (wait time for next fare)\n",
    "df_selected = df_selected.withColumn(\"wait_time\", col(\"next_pickup_ts\") - col(\"dropoff_ts\"))\n",
    "\n",
    "df_selected = df_selected.filter(col(\"wait_time\").isNotNull())\n",
    "\n",
    "# Show results\n",
    "df_selected.select(\"medallion\", \"dropoff_borough\", \"dropoff_ts\", \"next_pickup_ts\", \"wait_time\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------------+\n",
      "|dropoff_borough|     avg_wait_time|\n",
      "+---------------+------------------+\n",
      "|         Queens|           6423.75|\n",
      "|        Unknown|12119.191374663073|\n",
      "|       Brooklyn| 6589.554579673777|\n",
      "|  Staten Island|           13935.0|\n",
      "|      Manhattan|  2052.45890956484|\n",
      "|          Bronx| 4989.473684210527|\n",
      "+---------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "avg_wait_time = df_selected.groupBy(\"dropoff_borough\").agg(\n",
    "    avg(\"wait_time\").alias(\"avg_wait_time\")\n",
    ")\n",
    "\n",
    "# Show results\n",
    "avg_wait_time.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|dropoff_borough|count|\n",
      "+---------------+-----+\n",
      "|         Queens| 4832|\n",
      "|        Unknown| 1855|\n",
      "|       Brooklyn| 3188|\n",
      "|  Staten Island|   12|\n",
      "|      Manhattan|82866|\n",
      "|          Bronx|  361|\n",
      "+---------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_selected.groupBy(\"dropoff_borough\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
